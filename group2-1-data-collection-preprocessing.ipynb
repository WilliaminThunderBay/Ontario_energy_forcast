{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f755af-e7b7-4e1e-bc0f-da5229757f2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Forecasting Ontario's Energy Demand with Climate and Population Data  \n",
    "## Data Collection and Data Preprocessing Overview\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "<table style=\"margin-left: 0;\">\n",
    "  <tr>\n",
    "    <th>Dataset</th>\n",
    "    <th>Description</th>\n",
    "    <th>Source</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Energy Demand</td>\n",
    "    <td>Hourly electricity demand and price</td>\n",
    "    <td><a href=\"https://www.kaggle.com/code/pythonafroz/ontario-energy-demand-forecast-with-time-series/input\">Energy Demand Forecast</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Weather Data</td>\n",
    "    <td>Historical weather features from 10 stations</td>\n",
    "    <td><a href=\"https://climate.weather.gc.ca/historical_data/search_historic_data_stations_e.html?searchType=stnProv&timeframe=1&lstProvince=ON&optLimit=yearRange&StartYear=2003&EndYear=2023&Year=2025&Month=2&Day=17&selRowPerPage=25\">Gov. of Canada – Ontario</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Population</td>\n",
    "    <td>Quarterly population estimates</td>\n",
    "    <td><a href=\"https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1710000901&cubeTimeFrame.startMonth=01&cubeTimeFrame.startYear=2003&cubeTimeFrame.endMonth=10&cubeTimeFrame.endYear=2023&referencePeriods=20030101%2C20231001\">Statistics Canada</a></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Collection\n",
    "\n",
    "<table style=\"margin-left: 0;\">\n",
    "  <tr>\n",
    "    <th>Dataset</th>\n",
    "    <th>Method</th>\n",
    "    <th>Tool</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Energy Demand</td>\n",
    "    <td>CSV Download</td>\n",
    "    <td>Manual</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Weather Data</td>\n",
    "    <td>Web Scraping</td>\n",
    "    <td>Selenium</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Population</td>\n",
    "    <td>Web Scraping</td>\n",
    "    <td>Python</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Preprocessing\n",
    "\n",
    "<table style=\"margin-left: 0;\">\n",
    "  <tr>\n",
    "    <th>Dataset</th>\n",
    "    <th>Preprocessing Steps</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Energy Demand</td>\n",
    "    <td>Check for NaNs, sample-and-hold</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Weather Data</td>\n",
    "    <td>Drop duplicates, fill missing, circular mean for wind, average across stations</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Population</td>\n",
    "    <td>Check for NaNs, linear interpolation</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fbecc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ontario Weather Dataset Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9621a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T15:11:38.760357Z",
     "start_time": "2025-03-13T15:11:33.949238Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Environment Setup for Selenium with Google Chrome\n",
    "# ------------------------------\n",
    "\n",
    "# Update package list and install Google Chrome\n",
    "!apt update -y\n",
    "!apt install -y google-chrome-stable\n",
    "\n",
    "# Install Python package for managing WebDriver binaries\n",
    "!pip install webdriver-manager\n",
    "\n",
    "# Extract the installed version of Google Chrome (e.g., 121.x.x.x → 121)\n",
    "CHROME_VERSION = !google-chrome --version\n",
    "CHROME_VERSION = CHROME_VERSION[0].split()[2].split('.')[0]\n",
    "\n",
    "# Download and install the matching version of ChromeDriver\n",
    "!wget -q \"https://chromedriver.storage.googleapis.com/$(curl -sS https://chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION)/chromedriver-linux64.zip\"\n",
    "!unzip -o chromedriver-linux64.zip -d /usr/local/bin/\n",
    "!chmod +x /usr/local/bin/chromedriver\n",
    "\n",
    "# Confirm that ChromeDriver was installed correctly\n",
    "!chromedriver --version\n",
    "\n",
    "# Install the Selenium library\n",
    "!pip install selenium\n",
    "\n",
    "# ------------------------------\n",
    "# Selenium WebDriver Configuration\n",
    "# ------------------------------\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Define paths to the Chrome binary and ChromeDriver\n",
    "chrome_path = \"/usr/bin/google-chrome\"\n",
    "chromedriver_path = \"/usr/local/bin/chromedriver\"\n",
    "\n",
    "# Configure Chrome options\n",
    "options = Options()\n",
    "options.binary_location = chrome_path\n",
    "# options.add_argument(\"--headless\")  # Uncomment to run Chrome in headless mode\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Set up the ChromeDriver service\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# ------------------------------\n",
    "# Alternative Setup (Full Keyring + Repository Installation)\n",
    "# ------------------------------\n",
    "\n",
    "# Update system and install dependencies for secure package handling\n",
    "!apt-get update\n",
    "!apt-get install -y wget unzip\n",
    "\n",
    "# Add Google Chrome’s signing key and repository manually\n",
    "!wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | gpg --dearmor > /usr/share/keyrings/google-chrome-keyring.gpg\n",
    "!echo 'deb [signed-by=/usr/share/keyrings/google-chrome-keyring.gpg] http://dl.google.com/linux/chrome/deb/ stable main' | tee /etc/apt/sources.list.d/google-chrome.list\n",
    "\n",
    "# Install Google Chrome (again, from newly added repo)\n",
    "!apt-get update\n",
    "!apt-get install -y google-chrome-stable\n",
    "\n",
    "# Download the latest ChromeDriver release (general version)\n",
    "!wget https://chromedriver.storage.googleapis.com/$(curl -sS https://chromedriver.storage.googleapis.com/LATEST_RELEASE)/chromedriver_linux64.zip\n",
    "!unzip chromedriver_linux64.zip\n",
    "!mv chromedriver /usr/local/bin/\n",
    "!chmod +x /usr/local/bin/chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6fd612-1d16-4fe8-8085-745e71a66edd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T15:26:33.982634Z",
     "start_time": "2025-03-13T15:26:19.733643Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "import os\n",
    "import zipfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select, WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def get_chrome_driver():\n",
    "    \"\"\"\n",
    "    Configures and returns a Chrome WebDriver instance with predefined options.\n",
    "    Sets the default download directory and disables GPU/Dev Shm for compatibility.\n",
    "    \"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # Headless mode is disabled for debugging; uncomment if needed\n",
    "    # options.add_argument(\"--headless=new\")\n",
    "    \n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "    # Define Chrome's default download behavior\n",
    "    download_dir = os.getcwd()\n",
    "    prefs = {\n",
    "        \"download.default_directory\": download_dir,\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True,\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "def wait_for_download(directory, timeout=30):\n",
    "    \"\"\"\n",
    "    Waits for all Chrome .crdownload files to complete download within a given timeout.\n",
    "    \"\"\"\n",
    "    time_waited = 0\n",
    "    while any(fname.endswith(\".crdownload\") for fname in os.listdir(directory)):\n",
    "        if time_waited > timeout:\n",
    "            print(\"Warning: Timeout while waiting for file download.\")\n",
    "            break\n",
    "        time.sleep(1)\n",
    "        time_waited += 1\n",
    "\n",
    "def download_data_for_url(url, station_name):\n",
    "    \"\"\"\n",
    "    Automates download of monthly weather data from a specified URL and station name.\n",
    "    Downloads all available months between 2003 and 2023 and compresses the result.\n",
    "    \"\"\"\n",
    "    driver = None\n",
    "    try:\n",
    "        station_dir = os.path.join(os.getcwd(), station_name)\n",
    "        os.makedirs(station_dir, exist_ok=True)\n",
    "\n",
    "        driver = get_chrome_driver()\n",
    "\n",
    "        # Allow downloads via Chrome DevTools Protocol\n",
    "        driver.command_executor._commands[\"send_command\"] = (\"POST\", \"/session/$sessionId/chromium/send_command\")\n",
    "        params = {\n",
    "            \"cmd\": \"Page.setDownloadBehavior\",\n",
    "            \"params\": {\"behavior\": \"allow\", \"downloadPath\": station_dir},\n",
    "        }\n",
    "        driver.execute(\"send_command\", params)\n",
    "\n",
    "        wait = WebDriverWait(driver, 40)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Allow full page load\n",
    "\n",
    "        # Optional: Debug info for <select> elements and iframes\n",
    "        select_elements = driver.find_elements(By.TAG_NAME, \"select\")\n",
    "        print(f\"Found {len(select_elements)} <select> elements.\")\n",
    "        for idx, select in enumerate(select_elements):\n",
    "            print(f\"Select {idx}: {select.get_attribute('outerHTML')}\")\n",
    "\n",
    "        iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "        print(f\"Found {len(iframes)} iframes.\")\n",
    "        if iframes:\n",
    "            driver.switch_to.frame(iframes[0])\n",
    "            print(\"Switched to first iframe.\")\n",
    "\n",
    "        # Locate and interact with the year dropdown\n",
    "        year_container = wait.until(EC.presence_of_element_located((By.XPATH, \"//select[contains(@id, 'Year')]\")))\n",
    "        year_select = Select(year_container)\n",
    "        valid_years = [y.text.strip() for y in year_select.options if 2003 <= int(y.text.strip()) <= 2023]\n",
    "        print(f\"Years to process: {valid_years}\")\n",
    "\n",
    "        for year_text in valid_years:\n",
    "            year_container = wait.until(EC.presence_of_element_located((By.XPATH, \"//select[contains(@id, 'Year')]\")))\n",
    "            year_select = Select(year_container)\n",
    "            year_select.select_by_visible_text(year_text)\n",
    "\n",
    "            for month_index in range(12):  # Loop through all months\n",
    "                try:\n",
    "                    month_container = wait.until(EC.presence_of_element_located((By.XPATH, \"//select[contains(@id, 'Month')]\")))\n",
    "                    month_select = Select(month_container)\n",
    "                    month_option = month_select.options[month_index]\n",
    "                    month_text = month_option.text.strip()\n",
    "\n",
    "                    month_select.select_by_visible_text(month_text)\n",
    "                    print(f\"Processing: {year_text} - {month_text}\")\n",
    "\n",
    "                    # Click \"Go\" to load data\n",
    "                    go_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@type='submit' and contains(@class, 'btn-primary')]\")))\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", go_button)\n",
    "                    go_button.click()\n",
    "\n",
    "                    # Click \"Download Data\"\n",
    "                    download_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//input[@type='submit' and @value='Download Data']\")))\n",
    "                    download_button.click()\n",
    "\n",
    "                    # Wait until the download is complete\n",
    "                    wait_for_download(station_dir)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during download for {year_text} - {month_text}: {e}\")\n",
    "                    continue  # Continue with next month\n",
    "\n",
    "        # Create a ZIP archive of the station's downloaded CSVs\n",
    "        zip_path = os.path.join(os.getcwd(), f\"{station_name}_downloads.zip\")\n",
    "        with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
    "            for file in os.listdir(station_dir):\n",
    "                file_path = os.path.join(station_dir, file)\n",
    "                if file.endswith(\".csv\"):\n",
    "                    zipf.write(file_path, os.path.basename(file_path))\n",
    "\n",
    "        print(f\"Data for {station_name} compressed to {zip_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed for {station_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"Closing WebDriver...\")\n",
    "            driver.quit()\n",
    "            print(\"WebDriver closed.\")\n",
    "\n",
    "# Update the stations here, for example Guelph\n",
    "stations = {\n",
    "    \"Guelph\": \"https://climate.weather.gc.ca/climate_data/hourly_data_e.html?hlyRange=2006-12-15%7C2022-04-12&dlyRange=2006-09-28%7C2022-04-12&mlyRange=2006-12-01%7C2006-12-01&StationID=45407&Prov=ON&urlExtension=_e.html&searchType=stnProv&optLimit=yearRange&StartYear=2003&EndYear=2023&selRowPerPage=25&Line=126&Month=4&Day=12&lstProvince=ON&timeframe=1&Year=2022\"\n",
    "}\n",
    "\n",
    "# Loop through each station and trigger download\n",
    "for station_name, url in stations.items():\n",
    "    print(f\"\\nStarting download for: {station_name}\")\n",
    "    download_data_for_url(url, station_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd5c2c-fe06-4274-a7fb-47dfd92bf06f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:20:30.249797Z",
     "start_time": "2025-03-13T14:20:30.244871Z"
    },
    "id": "40dd5c2c-fe06-4274-a7fb-47dfd92bf06f"
   },
   "outputs": [],
   "source": [
    "# Make sure that you are inside the folder\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"fill-missing-data\")  # Change to the inner folder\n",
    "print(os.getcwd())  # Verify the new working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rjg9Lpc71Y_N",
   "metadata": {
    "id": "rjg9Lpc71Y_N",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ontario Weather Dataset Imputation and Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a1b680-13d1-4732-ad4f-d8700f9f9cf8",
   "metadata": {},
   "source": [
    "This script performs the following tasks:  \n",
    "\n",
    "### 1. **Set the Working Directory**  \n",
    "- Ensures the script runs from the correct folder (**`fill-missing-data`**).  \n",
    "- Uses `os.chdir()` to change to the correct working directory.  \n",
    "- Verifies the new directory with `os.getcwd()`.  \n",
    "\n",
    "### 2. **Install Required Libraries**  \n",
    "- Installs `pandas` (for data manipulation) and `scipy` (for statistical calculations).  \n",
    "- Uses `!pip install pandas scipy` to ensure dependencies are available.  \n",
    "\n",
    "### 3. **Handle Missing Weather Data**  \n",
    "- Uses `impute_missing_data(df)` to clean and fill missing values.  \n",
    "- Applies **linear interpolation** for temperature, dew point, and wind speed.  \n",
    "- Uses **forward-fill and backward-fill** for relative humidity and station pressure.  \n",
    "- Imputes missing wind direction using **circular mean interpolation**.  \n",
    "- Ensures **realistic temperature and wind speed ranges** to prevent outliers.  \n",
    "- Replaces zeros in key meteorological columns with `NaN` and fills them with appropriate values.  \n",
    "\n",
    "### 4. **Process and Save Cleaned Data**  \n",
    "- `process_all_stations(file_paths, output_dir)` reads multiple datasets.  \n",
    "- The function imputes missing values and saves the cleaned data into **`../imputed_datasets`**.  \n",
    "\n",
    "### 5. **Example Usage**  \n",
    "- The script processes a list of CSV files, including **`Consolidated_Toronto.csv`**.  \n",
    "- The output files are saved with imputed values in the specified directory.  \n",
    "\n",
    "### Note  \n",
    "- Ensure the script is executed in the correct folder (**`fill-missing-data`**).  \n",
    "- All CSV files should exist in the working directory before running.  \n",
    "- The imputation logic prevents unrealistic weather conditions from affecting the dataset.  \n",
    "\n",
    "### Workflow  \n",
    "- `impute_missing_data(df)`: Cleans missing values using appropriate interpolation techniques.  \n",
    "- `process_all_stations(file_paths, output_dir)`: Processes multiple station datasets and saves the cleaned versions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232b2080455ac97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:20:36.475729Z",
     "start_time": "2025-03-13T14:20:35.826734Z"
    },
    "id": "4232b2080455ac97",
    "outputId": "9139158d-955e-4e96-99a2-ec03fee19411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/ELEC390_Lab1/lib/python3.12/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/ELEC390_Lab1/lib/python3.12/site-packages (1.15.2)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/envs/ELEC390_Lab1/lib/python3.12/site-packages (from pandas) (2.0.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/ELEC390_Lab1/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/ELEC390_Lab1/lib/python3.12/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/ELEC390_Lab1/lib/python3.12/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ELEC390_Lab1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (pandas for data manipulation, scipy for statistical and interpolation functions)\n",
    "!pip install pandas scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VOfERWaW1kBZ",
   "metadata": {
    "id": "VOfERWaW1kBZ"
   },
   "source": [
    "### `impute_missing_data(df)`  \n",
    "**Purpose:** Handles missing weather data imputation.  \n",
    "\n",
    "**How it Works:**  \n",
    "- Uses **linear interpolation** for temperature, dew point, and wind speed.  \n",
    "- Applies **forward-fill and backward-fill** for relative humidity and station pressure.  \n",
    "- Uses **circular mean interpolation** for wind direction.  \n",
    "- Ensures **realistic temperature and wind speed ranges** to prevent outliers.  \n",
    "- Replaces zeros in key meteorological columns with `NaN` and fills them appropriately.  \n",
    "\n",
    "**What to Look For:**  \n",
    "- Missing values should be interpolated smoothly without unrealistic jumps.  \n",
    "- Ensure imputed values align with expected weather trends.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a4e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:25:59.494234Z",
     "start_time": "2025-03-13T14:25:58.370950Z"
    },
    "collapsed": true,
    "id": "initial_id",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "042d1d54-44a7-482a-8215-1546bade4291"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1b/gz7s8qf92rg37q344skx3cmw0000gn/T/ipykernel_96446/853362762.py:85: DtypeWarning: Columns (10,12,14,16,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/opt/anaconda3/envs/ELEC390_Lab1/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/var/folders/1b/gz7s8qf92rg37q344skx3cmw0000gn/T/ipykernel_96446/853362762.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['Unknown' 'Unknown' 'Unknown' ... 'Unknown' 'Unknown' 'Unknown']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, 'Weather'] = df['Weather'].fillna(most_frequent_weather)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed data saved to: ../imputed_datasets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import circmean\n",
    "import os\n",
    "\n",
    "def impute_missing_data(df):\n",
    "    # Identify existing columns for dropping metadata rows\n",
    "    # Ensure at least some valid values exist before interpolation\n",
    "    # This step handles missing values for temperature, dew point temperature, and wind speed\n",
    "    # by using linear interpolation. The method helps fill in gaps in a time series dataset,\n",
    "    # ensuring smoother transitions and preventing data loss.\n",
    "    required_columns = ['Temp (°C)', 'Dew Point Temp (°C)', 'Wind Spd (km/h)', 'Rel Hum (%)', 'Stn Press (kPa)', 'Visibility (km)', 'Wind Dir (10s deg)', 'Weather']\n",
    "    existing_columns = [col for col in required_columns if col in df.columns]\n",
    "\n",
    "    # Remove rows that only contain metadata (no weather-related data)\n",
    "    if existing_columns:\n",
    "        df = df.dropna(how='all', subset=existing_columns)\n",
    "\n",
    "    # Ensure at least some valid values exist before interpolation\n",
    "    for col in ['Temp (°C)', 'Dew Point Temp (°C)', 'Wind Spd (km/h)']:\n",
    "        if col in df.columns and df[col].notna().sum() > 1:\n",
    "            df.loc[:, col] = df[col].interpolate(method='linear', limit_direction='both')\n",
    "            df.loc[:, col] = df[col].bfill().ffill().round(2)\n",
    "\n",
    "    # Ensure temperature is within a realistic seasonal range (-40 to 40°C)\n",
    "    # This prevents unrealistic temperature readings, which could arise due to faulty sensors or data errors.\n",
    "    if 'Temp (°C)' in df.columns:\n",
    "        df.loc[:, 'Temp (°C)'] = df['Temp (°C)'].clip(lower=-40, upper=40)\n",
    "\n",
    "    # Ensure wind speed is within a reasonable range (1 to 100 km/h)\n",
    "    # Wind speed should be non-negative and within a typical range to prevent incorrect calculations.\n",
    "    if 'Wind Spd (km/h)' in df.columns:\n",
    "        df.loc[:, 'Wind Spd (km/h)'] = df['Wind Spd (km/h)'].clip(lower=1, upper=100)\n",
    "        df.loc[:, 'Wind Spd (km/h)'] = df['Wind Spd (km/h)'].fillna(df['Wind Spd (km/h)'].median()).round(2)\n",
    "\n",
    "    # Forward and Backward Fill for Relative Humidity and Station Pressure\n",
    "    # These meteorological variables should not have zero values, so we replace zeros with NaN and use\n",
    "    # forward and backward filling to ensure continuity in the dataset.\n",
    "    for col in ['Rel Hum (%)', 'Stn Press (kPa)']:\n",
    "        if col in df.columns:\n",
    "            df.loc[:, col] = df[col].replace(0, np.nan)\n",
    "            df.loc[:, col] = df[col].bfill().ffill().round(2)\n",
    "\n",
    "    # Circular Mean Imputation for Wind Direction\n",
    "    # Wind direction is a circular variable (0° and 360° are equivalent).\n",
    "    # We convert it to radians, interpolate missing values, and convert it back to degrees to maintain accuracy.\n",
    "    if 'Wind Dir (10s deg)' in df.columns:\n",
    "        df.loc[:, 'Wind Dir (10s deg)'] = df['Wind Dir (10s deg)'].replace(0, np.nan)\n",
    "        df.loc[:, 'Wind Dir (10s deg)'] = df['Wind Dir (10s deg)'].apply(lambda x: np.deg2rad(x) if pd.notnull(x) else np.nan)\n",
    "        df.loc[:, 'Wind Dir (10s deg)'] = df['Wind Dir (10s deg)'].interpolate()\n",
    "        df.loc[:, 'Wind Dir (10s deg)'] = df['Wind Dir (10s deg)'].apply(lambda x: np.rad2deg(x)).round(2)\n",
    "\n",
    "    # Location-Based Mean for Visibility\n",
    "    # Visibility values should not be zero. We replace zero with NaN and fill missing values using the median,\n",
    "    # which is a robust method for handling outliers.\n",
    "    if 'Visibility (km)' in df.columns:\n",
    "        df.loc[:, 'Visibility (km)'] = df['Visibility (km)'].replace(0, np.nan)\n",
    "        df.loc[:, 'Visibility (km)'] = df['Visibility (km)'].fillna(df['Visibility (km)'].median()).round(2)\n",
    "\n",
    "    # Wind Chill Calculation (Fixing large negative values issue)\n",
    "    # Wind chill is calculated when temperature and wind speed are available.\n",
    "    # This formula helps estimate how cold it feels due to wind, preventing unrealistic extreme values.\n",
    "    if 'Wind Chill' in df.columns and 'Temp (°C)' in df.columns and 'Wind Spd (km/h)' in df.columns:\n",
    "        mask = df['Wind Chill'].isna()\n",
    "        valid_wind_mask = df['Wind Spd (km/h)'] > 0\n",
    "        valid_mask = mask & valid_wind_mask\n",
    "        df.loc[valid_mask, 'Wind Chill'] = 13.12 + 0.6215 * df.loc[valid_mask, 'Temp (°C)'] - 11.37 * (df.loc[valid_mask, 'Wind Spd (km/h)'] ** 0.16) + 0.3965 * df.loc[valid_mask, 'Temp (°C)'] * (df.loc[valid_mask, 'Wind Spd (km/h)'] ** 0.16)\n",
    "        df.loc[:, 'Wind Chill'] = df['Wind Chill'].bfill().ffill().round(2)\n",
    "\n",
    "    # Mode Imputation for Weather Condition\n",
    "    # Since weather conditions are categorical, we use mode (most frequently occurring value) to fill missing values.\n",
    "    # If mode() is empty (all values missing), we default to 'Unknown'.\n",
    "    if 'Weather' in df.columns:\n",
    "        most_frequent_weather = df['Weather'].mode()[0] if not df['Weather'].mode().empty else 'Unknown'\n",
    "        df.loc[:, 'Weather'] = df['Weather'].fillna(most_frequent_weather)\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_all_stations(file_paths, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file)\n",
    "        df_imputed = impute_missing_data(df)\n",
    "        output_file = os.path.join(output_dir, os.path.basename(file))\n",
    "        df_imputed.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Imputed data saved to: {output_dir}\")\n",
    "\n",
    "# Example usage:\n",
    "file_paths = [\n",
    "    # \"Consolidated_Algonquin.csv\",\n",
    "    # \"Consolidated_Guelph.csv\",\n",
    "    # \"Consolidated_KitchenerWaterloo.csv\",\n",
    "    # \"Consolidated_LakeSuperiorProvincialPark.csv\",\n",
    "    # \"Consolidated_London.csv\",\n",
    "    #\"Consolidated_NorthernOntario.csv\",\n",
    "    # \"Consolidated_Ottawa.csv\",\n",
    "    # \"Consolidated_ThunderBay.csv\",\n",
    "    # \"Consolidated_Toronto.csv\",\n",
    "    # \"Consolidated_Windsor.csv\"\n",
    "    \"Consolidated_Toronto.csv\"\n",
    "]\n",
    "\n",
    "process_all_stations(file_paths, \"../imputed_datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d1b21373d5bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the electricity demand dataset (CSV format)\n",
    "df_csv = pd.read_csv(\"Ontario_Electricity_Demand_with_Population.csv\")\n",
    "print(\"CSV data loaded successfully.\")\n",
    "display(df_csv.head())\n",
    "\n",
    "# Load the consolidated dataset (Excel format)\n",
    "df_excel = pd.read_excel(\"Consolidated_Ontario_Final (1).xlsx\")\n",
    "print(\"Excel data loaded successfully.\")\n",
    "display(df_excel.head())\n",
    "\n",
    "# Identify common columns between the two datasets for merging\n",
    "common_columns = list(set(df_csv.columns) & set(df_excel.columns))\n",
    "print(\"Common columns identified for merge:\", common_columns)\n",
    "\n",
    "# Proceed to merge only if common columns are found\n",
    "if common_columns:\n",
    "    # Perform an inner merge on the first common column found\n",
    "    df_merged = pd.merge(df_csv, df_excel, on=common_columns[0], how='inner')\n",
    "    print(\"Datasets merged successfully.\")\n",
    "    display(df_merged.head())\n",
    "\n",
    "    # Sort the merged data by date column if present\n",
    "    date_column = 'Date'  # Update if your actual date column uses a different name\n",
    "    if date_column in df_merged.columns:\n",
    "        df_merged[date_column] = pd.to_datetime(df_merged[date_column])\n",
    "        df_merged = df_merged.sort_values(by=date_column)\n",
    "        print(f\"Merged data sorted by '{date_column}'.\")\n",
    "        display(df_merged.head())\n",
    "    else:\n",
    "        print(\"Note: No 'Date' column found for sorting.\")\n",
    "else:\n",
    "    print(\"Warning: No common columns found for merging.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637fa1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ontario Population Dataset Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22d46d-45f3-456a-9e1e-d8dd47e1e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install selenium pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb5de95-b8d4-4365-a28a-326b0adfa1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Initialize Headless Chrome WebDriver\n",
    "# --------------------------------------------------------\n",
    "# Ensure that ChromeDriver is installed and available in your system PATH\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode (no GUI)\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Navigate to Statistics Canada - Ontario Population Table\n",
    "# --------------------------------------------------------\n",
    "url = (\n",
    "    \"https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?\"\n",
    "    \"pid=1710000901&cubeTimeFrame.startMonth=01&cubeTimeFrame.startYear=2003&\"\n",
    "    \"cubeTimeFrame.endMonth=10&cubeTimeFrame.endYear=2023&\"\n",
    "    \"referencePeriods=20030101%2C20231001\"\n",
    ")\n",
    "driver.get(url)\n",
    "\n",
    "# Allow time for the interactive table to fully render\n",
    "time.sleep(5)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Scrape Table Content for Ontario\n",
    "# --------------------------------------------------------\n",
    "table = driver.find_element(By.CLASS_NAME, \"T1__table\")\n",
    "rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "ontario_data = []\n",
    "headers = []\n",
    "\n",
    "for row in rows:\n",
    "    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    if not cells:\n",
    "        # Extract column headers from the header row\n",
    "        headers = [th.text for th in row.find_elements(By.TAG_NAME, \"th\") if th.text]\n",
    "    else:\n",
    "        geo = cells[0].text.strip()\n",
    "        if \"Ontario\" in geo:\n",
    "            # Extract Ontario's quarterly population values\n",
    "            ontario_data = [cell.text.replace(\",\", \"\") for cell in cells[1:]]\n",
    "            break\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Construct DataFrame for Ontario Population\n",
    "# --------------------------------------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"Quarter\": headers[1:],  # Skip the first header (\"Geography\")\n",
    "    \"Population\": ontario_data\n",
    "})\n",
    "\n",
    "# Convert population values to numeric format\n",
    "df[\"Population\"] = pd.to_numeric(df[\"Population\"])\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Export to CSV\n",
    "# --------------------------------------------------------\n",
    "df.to_csv(\"ontario_population_2003_2023.csv\", index=False)\n",
    "print(\"✅ Ontario population data saved to 'ontario_population_2003_2023.csv'.\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Close WebDriver\n",
    "# --------------------------------------------------------\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac041da6-8d97-46a0-809b-e1ceb1fc3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a89ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T21:53:25.136203Z",
     "start_time": "2025-03-14T21:53:24.790254Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def preprocess_population_data():\n",
    "    file_path = \"ontario_population.xlsx\"\n",
    "    output_path = \"ontario_population_processed.csv\"\n",
    "    sheet_name = \"ontario_population\"\n",
    "\n",
    "    # Load the Excel file\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    df = xls.parse(sheet_name)\n",
    "\n",
    "    # Identify the row index where the actual data starts\n",
    "    header_index = None\n",
    "    for i, row in df.iterrows():\n",
    "        if \"Geography\" in str(row[0]):  # Look for the header row containing \"Geography\"\n",
    "            header_index = i\n",
    "            break\n",
    "\n",
    "    if header_index is None:\n",
    "        raise ValueError(\"Could not find the header row containing 'Geography'\")\n",
    "\n",
    "    # Re-load the data with the correct header\n",
    "    df_cleaned = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=header_index)\n",
    "\n",
    "    # Drop any completely empty columns\n",
    "    df_cleaned = df_cleaned.dropna(how='all', axis=1)\n",
    "\n",
    "    # Reset column names to the first row and remove it from the data\n",
    "    df_cleaned.columns = df_cleaned.iloc[0]\n",
    "    df_cleaned = df_cleaned[1:].reset_index(drop=True)\n",
    "\n",
    "    # Extract only the relevant row containing Ontario population data\n",
    "    df_population = df_cleaned[df_cleaned.iloc[:, 0] == \"Ontario\"].set_index(df_cleaned.columns[0])\n",
    "\n",
    "    # Transpose the data to make quarters a column\n",
    "    df_melted = df_population.T.reset_index()\n",
    "    df_melted.columns = [\"Quarter\", \"Population\"]\n",
    "\n",
    "    # Remove any rows with missing population values\n",
    "    df_melted[\"Population\"] = df_melted[\"Population\"].interpolate(method=\"linear\")\n",
    "\n",
    "    # Convert population values to integers\n",
    "    df_melted[\"Population\"] = df_melted[\"Population\"].astype(int)\n",
    "\n",
    "    # Save the processed data to CSV\n",
    "    df_melted.to_csv(output_path, index=False)\n",
    "    print(f\"Processed data saved to {output_path}\")\n",
    "\n",
    "# Run the function\n",
    "preprocess_population_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee706eea-0d1d-4e08-b85a-13dcd136e234",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ontario Energy Demand Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5558b-f134-43f6-95bf-4fd2871a8ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c767368-a887-4549-86d9-167646c8a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Load Ontario Electricity Demand Dataset\n",
    "# --------------------------------------------------------\n",
    "df = pd.read_csv(\"ontario_electricity_demand.csv\")  # Ensure this file is in the same directory or provide full path\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Check for Missing or Invalid Values\n",
    "# --------------------------------------------------------\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Apply Sample-and-Hold Technique (Forward Fill)\n",
    "# --------------------------------------------------------\n",
    "df_cleaned = df.fillna(method='ffill')\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Preview Cleaned Data\n",
    "# --------------------------------------------------------\n",
    "print(\"\\nCleaned Data Preview:\")\n",
    "display(df_cleaned.head())\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Save Cleaned Dataset to CSV\n",
    "# --------------------------------------------------------\n",
    "df_cleaned.to_csv(\"ontario_electricity_demand_cleaned.csv\", index=False)\n",
    "print(\"\\n Cleaned dataset saved to 'ontario_electricity_demand_cleaned.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb1baa-1525-4a2e-9e07-b5f1dd3fd8c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Merge All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a0c6d-1edc-4e35-b310-51a208143c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Step 1: Load Electricity Demand and Consolidated Datasets\n",
    "# --------------------------------------------------------\n",
    "# Load the dataset containing electricity demand and population data\n",
    "df_demand = pd.read_csv(\"Ontario_Electricity_Demand_with_Population.csv\")\n",
    "\n",
    "# Load the consolidated dataset, which may include weather or processed station data\n",
    "df_consolidated = pd.read_excel(\"Consolidated_Ontario_Final.xlsx\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Step 2: Identify Common Columns for Merging\n",
    "# --------------------------------------------------------\n",
    "# Determine shared columns between the two datasets to use as join keys\n",
    "common_columns = list(set(df_demand.columns) & set(df_consolidated.columns))\n",
    "print(\"Common column(s) identified for merge:\", common_columns)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Step 3: Merge Datasets Based on Shared Column\n",
    "# --------------------------------------------------------\n",
    "if common_columns:\n",
    "    # Perform an inner join on the first shared column\n",
    "    merged_df = pd.merge(df_demand, df_consolidated, on=common_columns[0], how='inner')\n",
    "    print(f\"Datasets merged using key column: '{common_columns[0]}'\")\n",
    "    display(merged_df.head())\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Step 4: Sort Merged Data by Date Column\n",
    "    # ----------------------------------------------------\n",
    "    if 'Date' in merged_df.columns:\n",
    "        # Convert the 'Date' column to datetime format\n",
    "        merged_df['Date'] = pd.to_datetime(merged_df['Date'], errors='coerce')\n",
    "        merged_df = merged_df.sort_values(by='Date')\n",
    "        print(\"Merged data sorted by 'Date' column.\")\n",
    "        display(merged_df.head())\n",
    "    else:\n",
    "        print(\"No 'Date' column found in merged data. Sorting step skipped.\")\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Step 5: Export Merged Dataset to CSV\n",
    "    # ----------------------------------------------------\n",
    "    merged_df.to_csv(\"Merged_Ontario_Energy_Dataset.csv\", index=False)\n",
    "    print(\"Merged dataset saved as 'Merged_Ontario_Energy_Dataset.csv'.\")\n",
    "\n",
    "else:\n",
    "    print(\"No common columns found between the datasets. Merge operation skipped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
